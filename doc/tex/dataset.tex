\section{Dados de entrada}
\label{sec:dataset}

Para este exercício foi considerado um conjunto de dados com informação sobre sms spam\footnote{Extraido de http://www.dt.fee.unicamp.br/$\sim$tiago/smsspamcollection/}. O conjunto de dados tem 5559 instâncias e 2 características (ou atributos): Type (o tipo de sms: ham ou spam) e Text (o texto original do sms).

Da forma que o conjunto de dados se encontra, não é fácil usá-lo com um classificador porque o atributo Text é uma cadeia de texto muito diferente em cada instância do conjunto, mas poderia ser pré-processada de forma que seja um vetor de características como é definido a continuação:

\begin{itemize}
	\item Term Frequency (TF): Número de vezes que cada palavra aparece no texto do sms
		%\[ {TF}_i( {word} ) = \sum_{j=0}^{|P^i|} P^i_j = {word} \]
	\item Tf\_Idf: Relevância da palavra na instância comparada com o conjunto total
		\[ {Tf\_Idf}( {word} ) = TF( {word} ) * log( N / DF( {word} ) ) \]
		%\[ DF( {word} ) = \sum_{i=0}^N {has\_word}( P^i , {word} ) \]
\end{itemize}
onde $N$ é o número total de instâncias. Então o tamanho do vetor de características vai depender do número total de palavras no conjunto, mas isso gera os seguintes problemas para fazer o pré-processamento:
\begin{itemize}
	\item Palavras muito parecidas ou que são conjugações de outras (e.g. rise-rises, eat-ate)
	\item Muitas abreviações de palavras (e.g. u-you, vry-very)
	\item Palavras que aparecem en quase todos os textos ou em quase nenhum (também chamadas stop-words)
\end{itemize}
Para solucionar cada um dos problemas foram desenvolvidas as seguintes soluções:
\begin{itemize}
	\item Foi usada a livraria NLTK (Python)\footnote{Official site: http://www.nltk.org} para fazer lemmatization (mudar a palavra a uma conjugação padrão) e stemming (encontrar a raiz da palavra)
	\item Foi usado um arquivo com sentenças equivalentes onde cada abreviação tinha um conjunto de palavras assignado\footnote{O arquivo está em: https://github.com/NonWhite/IA\_EP3/blob/master/data/equivalents.txt}
	\item Foi usado um arquivo com as palavras que não dam muita informação sobre o texto, como pronomes ou interjeições\footnote{O arquivo está em: https://github.com/NonWhite/IA\_EP3/blob/master/data/stop\_words.txt}
\end{itemize}

Por último, os conjuntos de dados novos (com representação de vetor de características) foram divididos em duas partes: treinamento (70\%) e validação (30\%) para os experimentos das seguintes seções.